{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 100\n",
    "img_width = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8998 files belonging to 5 classes.\n",
      "Using 7199 files for training.\n",
      "Found 8998 files belonging to 5 classes.\n",
      "Using 1799 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  './data/data/',\n",
    "  validation_split=0.2,\n",
    "    color_mode=\"grayscale\",\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  './data/data/',\n",
    "  validation_split=0.2,\n",
    "  color_mode=\"grayscale\",\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['center', 'down', 'left', 'right', 'up']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(): \n",
    "    num_classes = 5\n",
    "\n",
    "    model = Sequential([\n",
    "      layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 1)),\n",
    "      layers.Conv2D(16, 1, padding='same', activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Conv2D(32, 1, padding='same', activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Conv2D(64, 1, padding='same', activation='relu'),\n",
    "      layers.MaxPooling2D(),\n",
    "      layers.Dropout(0.2),\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(128, activation='relu'),\n",
    "      layers.Dense(num_classes)\n",
    "    ])\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", \n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "225/225 [==============================] - 22s 98ms/step - loss: 0.4692 - accuracy: 0.8383 - val_loss: 0.1362 - val_accuracy: 0.9700\n",
      "Epoch 2/5\n",
      "225/225 [==============================] - 17s 76ms/step - loss: 0.0744 - accuracy: 0.9787 - val_loss: 0.0878 - val_accuracy: 0.9778\n",
      "Epoch 3/5\n",
      "225/225 [==============================] - 17s 77ms/step - loss: 0.0532 - accuracy: 0.9857 - val_loss: 0.0680 - val_accuracy: 0.9811\n",
      "Epoch 4/5\n",
      "225/225 [==============================] - 17s 77ms/step - loss: 0.0435 - accuracy: 0.9871 - val_loss: 0.0568 - val_accuracy: 0.9833\n",
      "Epoch 5/5\n",
      "225/225 [==============================] - 17s 77ms/step - loss: 0.0383 - accuracy: 0.9894 - val_loss: 0.0545 - val_accuracy: 0.9850\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "epochs = 5\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "225/225 [==============================] - 17s 78ms/step - loss: 0.0296 - accuracy: 0.9921 - val_loss: 0.0531 - val_accuracy: 0.9855\n",
      "Epoch 2/5\n",
      "225/225 [==============================] - 17s 77ms/step - loss: 0.0300 - accuracy: 0.9915 - val_loss: 0.0437 - val_accuracy: 0.9900\n",
      "Epoch 3/5\n",
      "225/225 [==============================] - 17s 78ms/step - loss: 0.0224 - accuracy: 0.9936 - val_loss: 0.0363 - val_accuracy: 0.9900\n",
      "Epoch 4/5\n",
      "225/225 [==============================] - 17s 77ms/step - loss: 0.0229 - accuracy: 0.9936 - val_loss: 0.0343 - val_accuracy: 0.9900\n",
      "Epoch 5/5\n",
      "225/225 [==============================] - 17s 78ms/step - loss: 0.0223 - accuracy: 0.9933 - val_loss: 0.0471 - val_accuracy: 0.9900\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Models/mark3/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./Models/mark3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./data/data/left/left31.jpg',cv2.IMREAD_UNCHANGED)\n",
    "img = img.reshape((100,100,1))\n",
    "img = np.expand_dims(img,0)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['center', 'down', 'left', 'right', 'up']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 20 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fe44183d8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(img)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}